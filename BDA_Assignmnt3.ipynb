{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h2>Importing all the necessary modules</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0jYL-fRAGHli"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h2>Sampling</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "srkfcC_4GPyM"
      },
      "outputs": [],
      "source": [
        "def sample_json(input_file, output_file, target_size_gb, filter_key = 'also_buy'):\n",
        "  target_size_bytes = target_size_gb *1024 **3\n",
        "  current_size_bytes = 0\n",
        "\n",
        "  with open(input_file, 'r', encoding = 'utf-8') as infile, open(output_file, 'w', encoding = 'utf-8') as outfile:\n",
        "    for line in tqdm(infile):\n",
        "      record = json.loads(line)\n",
        "      if record.get(filter_key):\n",
        "        outfile.write(json.dumps(record) + '\\n')\n",
        "        current_size_bytes += len(line.encode('utf-8'))\n",
        "\n",
        "      if current_size_bytes >= target_size_bytes:\n",
        "        break\n",
        "\n",
        "  print(f\"Finished Sampling. Output size: {current_size_bytes / 1024** 3:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h2>Providing the json file</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KyBossXWG4sw"
      },
      "outputs": [],
      "source": [
        "sample_json = ('All_Amazon_Meta.json', 'Sampled_Amazon_Meta.json', 15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h2>Preprocessing and Cleaning</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Function to load JSON data\n",
        "def load_json(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        data = [json.loads(line) for line in file]\n",
        "    return data\n",
        "\n",
        "# Function to remove duplicates from data\n",
        "def remove_duplicates(data):\n",
        "    unique_data = [dict(t) for t in {tuple(d.items()) for d in data}]\n",
        "    return unique_data\n",
        "\n",
        "# Function to remove null values from data\n",
        "def remove_null_values(data, keys_to_check=None):\n",
        "    if keys_to_check:\n",
        "        cleaned_data = [record for record in data if all(record.get(key) is not None for key in keys_to_check)]\n",
        "    else:\n",
        "        cleaned_data = [record for record in data if all(value is not None for value in record.values())]\n",
        "    return cleaned_data\n",
        "\n",
        "# Tokenization and text cleaning function\n",
        "def tokenize_and_clean_text(text):\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "    # Remove non-alphanumeric characters\n",
        "    clean_tokens = [re.sub(r'[^a-zA-Z0-9]', '', token) for token in filtered_tokens]\n",
        "    # Remove empty tokens\n",
        "    clean_tokens = [token for token in clean_tokens if token]\n",
        "    return clean_tokens\n",
        "\n",
        "# Preprocess data function\n",
        "def preprocess_data(input_file, output_file):\n",
        "    # Load data\n",
        "    data = load_json(input_file)\n",
        "    \n",
        "    # Remove duplicates\n",
        "    data = remove_duplicates(data)\n",
        "    \n",
        "    # Remove null values\n",
        "    data = remove_null_values(data)\n",
        "    \n",
        "    # Tokenize and clean text fields (if any)\n",
        "    for record in data:\n",
        "        if 'text_field' in record:  # Adjust this according to your dataset\n",
        "            record['text_field'] = tokenize_and_clean_text(record['text_field'])\n",
        "    \n",
        "    # Save preprocessed data to a new JSON file\n",
        "    with open(output_file, 'w', encoding='utf-8') as file:\n",
        "        for record in data:\n",
        "            file.write(json.dumps(record) + '\\n')\n",
        "\n",
        "# Function for batch processing\n",
        "def batch_process(input_files_directory, output_directory):\n",
        "    input_files = os.listdir(input_files_directory)\n",
        "    for file_name in input_files:\n",
        "        if file_name.endswith('.json'):\n",
        "            input_file_path = os.path.join(input_files_directory, file_name)\n",
        "            output_file_path = os.path.join(output_directory, f'preprocessed_{file_name}')\n",
        "            preprocess_data(input_file_path, output_file_path)\n",
        "            print(f'Preprocessing completed for {file_name}')\n",
        "\n",
        "# Example usage:\n",
        "input_file = 'Sampled_Amazon_Meta.json'\n",
        "output_file = 'Preprocessed_Amazon_Meta.json'\n",
        "preprocess_data(input_file, output_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h2>Producer Application</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from confluent_kafka import Producer\n",
        "import json\n",
        "import time\n",
        "\n",
        "def produce_data(topic, data_file):\n",
        "    p = Producer({'bootstrap.servers': 'localhost:9092'})\n",
        "\n",
        "    with open(data_file, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            message = json.loads(line)\n",
        "            p.produce(topic, json.dumps(message))\n",
        "            time.sleep(0.1)  # Simulate real-time streaming\n",
        "            p.poll(0)\n",
        "\n",
        "    p.flush()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    topic = 'preprocessed_data'\n",
        "    data_file = 'Preprocessed_Amazon_Meta.json'\n",
        "    produce_data(topic, data_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h2>Consumer 1</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from confluent_kafka import Consumer, KafkaError\n",
        "\n",
        "def consume_data(topic):\n",
        "    c = Consumer({\n",
        "        'bootstrap.servers': 'localhost:9092',\n",
        "        'group.id': 'consumer_group_1',\n",
        "        'auto.offset.reset': 'earliest'\n",
        "    })\n",
        "    c.subscribe([topic])\n",
        "\n",
        "    while True:\n",
        "        msg = c.poll(1.0)\n",
        "\n",
        "        if msg is None:\n",
        "            continue\n",
        "        if msg.error():\n",
        "            if msg.error().code() == KafkaError._PARTITION_EOF:\n",
        "                continue\n",
        "            else:\n",
        "                print(msg.error())\n",
        "                break\n",
        "\n",
        "        print('Consumer 1:', msg.value().decode('utf-8'))\n",
        "\n",
        "    c.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    topic = 'preprocessed_data'\n",
        "    consume_data(topic)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h2>Consumer 2</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from confluent_kafka import Consumer, KafkaError\n",
        "\n",
        "def consume_data(topic):\n",
        "    c = Consumer({\n",
        "        'bootstrap.servers': 'localhost:9092',\n",
        "        'group.id': 'consumer_group_2',  # Different group ID\n",
        "        'auto.offset.reset': 'earliest'\n",
        "    })\n",
        "    c.subscribe([topic])\n",
        "\n",
        "    while True:\n",
        "        msg = c.poll(1.0)\n",
        "\n",
        "        if msg is None:\n",
        "            continue\n",
        "        if msg.error():\n",
        "            if msg.error().code() == KafkaError._PARTITION_EOF:\n",
        "                continue\n",
        "            else:\n",
        "                print(msg.error())\n",
        "                break\n",
        "\n",
        "        print('Consumer 2:', msg.value().decode('utf-8'))\n",
        "\n",
        "    c.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    topic = 'preprocessed_data'\n",
        "    consume_data(topic)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h2>Consumer 3</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from confluent_kafka import Consumer, KafkaError\n",
        "\n",
        "def consume_data(topic):\n",
        "    c = Consumer({\n",
        "        'bootstrap.servers': 'localhost:9092',\n",
        "        'group.id': 'consumer_group_3',  # Different group ID\n",
        "        'auto.offset.reset': 'earliest'\n",
        "    })\n",
        "    c.subscribe([topic])\n",
        "\n",
        "    while True:\n",
        "        msg = c.poll(1.0)\n",
        "\n",
        "        if msg is None:\n",
        "            continue\n",
        "        if msg.error():\n",
        "            if msg.error().code() == KafkaError._PARTITION_EOF:\n",
        "                continue\n",
        "            else:\n",
        "                print(msg.error())\n",
        "                break\n",
        "\n",
        "        print('Consumer 3:', msg.value().decode('utf-8'))\n",
        "\n",
        "    c.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    topic = 'preprocessed_data'\n",
        "    consume_data(topic)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h2>Consumer 1 Apriori </h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def apriori(stream, min_support):\n",
        "    item_counts = defaultdict(int)\n",
        "    frequent_itemsets = defaultdict(int)\n",
        "    \n",
        "    for transaction in stream:\n",
        "        # Update counts for single items\n",
        "        for item in transaction:\n",
        "            item_counts[item] += 1\n",
        "        \n",
        "        # Generate candidate itemsets\n",
        "        candidate_itemsets = set()\n",
        "        for item, count in item_counts.items():\n",
        "            if count >= min_support:\n",
        "                candidate_itemsets.add((item,))\n",
        "        \n",
        "        # Update counts for candidate itemsets\n",
        "        for itemset in candidate_itemsets:\n",
        "            for transaction in stream:\n",
        "                if all(item in transaction for item in itemset):\n",
        "                    frequent_itemsets[itemset] += 1\n",
        "    \n",
        "    # Output frequent itemsets\n",
        "    for itemset, count in frequent_itemsets.items():\n",
        "        if count >= min_support:\n",
        "            print(\"Frequent Itemset:\", itemset, \"Support:\", count)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h2>Consumer 2 PCY </h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def pcy(stream, min_support, hash_table_size):\n",
        "    item_counts = defaultdict(int)\n",
        "    hash_table = [0] * hash_table_size\n",
        "    frequent_itemsets = defaultdict(int)\n",
        "    \n",
        "    for transaction in stream:\n",
        "        # Update counts for single items\n",
        "        for item in transaction:\n",
        "            item_counts[item] += 1\n",
        "        \n",
        "        # Generate candidate item pairs\n",
        "        candidate_pairs = set()\n",
        "        for item1, count1 in item_counts.items():\n",
        "            if count1 >= min_support:\n",
        "                for item2, count2 in item_counts.items():\n",
        "                    if item1 != item2 and count2 >= min_support:\n",
        "                        candidate_pairs.add((item1, item2))\n",
        "        \n",
        "        # Update hash table\n",
        "        for item1, item2 in candidate_pairs:\n",
        "            hash_value = hash((item1, item2)) % hash_table_size\n",
        "            hash_table[hash_value] += 1\n",
        "        \n",
        "        # Update counts for frequent itemsets\n",
        "        for transaction in stream:\n",
        "            for item1, item2 in candidate_pairs:\n",
        "                hash_value = hash((item1, item2)) % hash_table_size\n",
        "                if hash_table[hash_value] >= min_support and item1 in transaction and item2 in transaction:\n",
        "                    frequent_itemsets[(item1, item2)] += 1\n",
        "    \n",
        "    # Output frequent itemsets\n",
        "    for itemset, count in frequent_itemsets.items():\n",
        "        if count >= min_support:\n",
        "            print(\"Frequent Itemset:\", itemset, \"Support:\", count)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
